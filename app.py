import streamlit as st
# from langchain_openai import ChatOpenAI
import os
import re
from LLM import Yuan2_LLM
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from zhipuai_embedding import ZhipuAIEmbeddings
from langchain_chroma import Chroma
from langchain.memory import ConversationBufferMemory
from transformers.utils import is_torch_cuda_available, is_torch_mps_available
from langchain.chains import ConversationalRetrievalChain
from dotenv import load_dotenv, find_dotenv
from logging_save import LoggingSave
_ = load_dotenv(find_dotenv())    # read local .env file


#export OPENAI_API_KEY=
#os.environ["OPENAI_API_BASE"] = 'https://api.chatgptid.net/v1'
zhipuai_api_key = os.environ['ZHIPUAI_API_KEY']
def has_more_than_one_chinese_question_mark(s):
    # åŒ¹é…ä¸­æ–‡é—®å·
    pattern = r'ï¼Ÿ'
    # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„ä¸­æ–‡é—®å·
    matches = re.findall(pattern, s)
    # åˆ¤æ–­åŒ¹é…çš„æ•°é‡æ˜¯å¦å¤§äº1
    return len(matches) > 1

def clean_text(text):
    # ç§»é™¤éæ–‡å­—å†…å®¹
    cleaned_text = re.sub(r'\[å›¾\d+\]', '', text)  # ç§»é™¤å½¢å¦‚ [å›¾1] çš„å†…å®¹
    cleaned_text = re.sub(r'\[å›¾\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [ å›¾ 1 ] çš„å†…å®¹
    cleaned_text = re.sub(r'\[å›¾\s*\d+\s*-\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [ å›¾ 1 - 2 ] çš„å†…å®¹
    cleaned_text = re.sub(r'\[å›¾\s*\d+\s*\.\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [ å›¾ 1 . 2 ] çš„å†…å®¹
    cleaned_text = re.sub(r'\[å‚è€ƒå›¾\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [é¢˜1] çš„å†…å®¹
    cleaned_text = re.sub(r'\[å‚è€ƒå›¾\s*\d+\s*-\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [é¢˜1-2] çš„å†…å®¹
    cleaned_text = re.sub(r'\[å‚è€ƒå›¾\s*\d+\s*\.\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [é¢˜1.2] çš„å†…å®¹
    cleaned_text = re.sub(r'\[è¡¨\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [é¢˜1] çš„å†…å®¹
    cleaned_text = re.sub(r'\[è¡¨\s*\d+\s*-\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [é¢˜1-2] çš„å†…å®¹
    cleaned_text = re.sub(r'\[è¡¨\s*\d+\s*\.\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [é¢˜1.2] çš„å†…å®¹
    cleaned_text = re.sub(r'\[è§è¡¨\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [é¢˜1] çš„å†…å®¹
    cleaned_text = re.sub(r'\[è§è¡¨\s*\d+\s*-\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [é¢˜1-2] çš„å†…å®¹
    cleaned_text = re.sub(r'\[è§è¡¨\s*\d+\s*\.\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [é¢˜1.2] çš„å†…å®¹
    cleaned_text = re.sub(r'\[\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [1] çš„å†…å®¹
    cleaned_text = re.sub(r'\[\d+\s*\-\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [1-2] çš„å†…å®¹
    cleaned_text = re.sub(r'\[\d+\s*\.\s*\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [1.2] çš„å†…å®¹
    cleaned_text = re.sub(r'\[\d+\s*\.\s*\d+\.\d+\]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [1.2] çš„å†…å®¹
    cleaned_text = re.sub(r'\[\Â·]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [1.2] çš„å†…å®¹
    cleaned_text = re.sub(r'\[\Â·\s*\Â·]', '', cleaned_text)  # ç§»é™¤å½¢å¦‚ [1.2] çš„å†…å®¹
    
    # cleaned_text = re.sub(r'\[.*?\]', '', cleaned_text)  # ç§»é™¤æ‰€æœ‰æ‹¬å·å†…çš„å†…å®¹
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)  # ç§»é™¤å¤šä½™çš„ç©ºæ ¼

    # æ£€æŸ¥æ˜¯å¦å…¨ä¸ºéä¸­æ–‡å­—ç¬¦
    if re.match(r'^[^\u4e00-\u9fff]*$', cleaned_text):
        cleaned_text = "è¯·ä¿é‡èº«ä½“ï¼"
    
    return cleaned_text

def get_first_part(text):
    truncated_text = text[:20]
    index_of_marker = truncated_text.find('ã€')
    if index_of_marker != -1:
        return truncated_text[:index_of_marker + 1]
    else:
        return truncated_text
    
def remove_trailing_character(s, char):
    return s.rstrip(char)

def generate_response(input_text):
    # llm = ChatOpenAI(temperature=0.7, openai_api_key=openai_api_key)
    llm = Yuan2_LLM('IEITYuan/Yuan2-2B-Mars-hf')
    output = llm.invoke(input_text)
    output_parser = StrOutputParser()
    output = output_parser.invoke(output)
    #st.info(output)
    return output

def generate_rag(question:str):
    vectordb = get_vectordb()
    docs = vectordb.max_marginal_relevance_search(question,k=3)
    str="\n".join([clean_text(i.page_content) for i in docs])
    if str:
        return str
    return "æŠ±æ­‰ï¼Œæš‚æ—¶æœªæ‰¾åˆ°ï¼"

def get_vectordb():
    # å®šä¹‰ Embeddings
    embedding = ZhipuAIEmbeddings()

    # å‘é‡æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
    persist_directory = 'database/vectordb'
    # åŠ è½½æ•°æ®åº“
    vectordb = Chroma(
        persist_directory=persist_directory,  # å…è®¸æˆ‘ä»¬å°†persist_directoryç›®å½•ä¿å­˜åˆ°ç£ç›˜ä¸Š
        embedding_function=embedding
    )
    return vectordb

#å¸¦æœ‰å†å²è®°å½•çš„é—®ç­”é“¾
def get_chat_qa_chain(question:str):
    vectordb = get_vectordb()
    # llm = ChatOpenAI(model_name = "gpt-3.5-turbo", temperature = 0,openai_api_key = openai_api_key)
    llm = Yuan2_LLM('IEITYuan/Yuan2-2B-Mars-hf')
    memory = ConversationBufferMemory(
        memory_key="chat_history",  # ä¸ prompt çš„è¾“å…¥å˜é‡ä¿æŒä¸€è‡´ã€‚
        return_messages=True  # å°†ä»¥æ¶ˆæ¯åˆ—è¡¨çš„å½¢å¼è¿”å›èŠå¤©è®°å½•ï¼Œè€Œä¸æ˜¯å•ä¸ªå­—ç¬¦ä¸²
    )
    retriever=vectordb.as_retriever()
    qa = ConversationalRetrievalChain.from_llm(
        llm,
        retriever=retriever,
        memory=memory
    )
    result = qa({"question": question})
    return result['answer']

#ä¸å¸¦å†å²è®°å½•çš„é—®ç­”é“¾
def get_qa_chain(question:str):
    vectordb = get_vectordb()
    # llm = ChatOpenAI(model_name = "gpt-3.5-turbo", temperature = 0,openai_api_key = openai_api_key)
    llm = Yuan2_LLM('IEITYuan/Yuan2-2B-Mars-hf')
    template = """ä½ æ˜¯ä¸€ä¸ªé«˜è¡€å‹å’Œå† å¿ƒç—…ä¸“å®¶ï¼Œè¯·ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›ç­”æœ€åçš„é—®é¢˜ã€‚å¦‚æœæ˜¯è‹±æ–‡ï¼Œå°±æŒ‰å•è¯åˆ†å‰²ä½¿ç”¨ç”¨åŒ»å­¦è¯­è¨€é£æ ¼ç¿»è¯‘åå†ç”¨ä¸­æ–‡å›ç­”ï¼Œå¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚ä½ åº”è¯¥ä½¿ç­”æ¡ˆå°½å¯èƒ½è¯¦ç»†å…·ä½“ï¼Œä½†ä¸è¦åé¢˜ã€‚å¦‚æœç­”æ¡ˆæ¯”è¾ƒé•¿ï¼Œè¯·é…Œæƒ…è¿›è¡Œåˆ†æ®µï¼Œä»¥æé«˜ç­”æ¡ˆçš„é˜…è¯»ä½“éªŒã€‚æ€»æ˜¯åœ¨å›ç­”çš„æœ€åè¯´â€œè°¢è°¢ä½ çš„æé—®ï¼â€ã€‚
        {context}
        é—®é¢˜: {question}
        """
    QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context","question"],
                                 template=template)
    qa_chain = RetrievalQA.from_chain_type(llm,
                                       retriever=vectordb.as_retriever(),
                                       return_source_documents=True,
                                       chain_type_kwargs={"prompt":QA_CHAIN_PROMPT})
    result = qa_chain({"query": question})
    # print('#'*60)
    # print(result["result"])
    # print(type(result["result"]))
    # print(len(result["result"]))
    # print('#'*60)
    flag=has_more_than_one_chinese_question_mark(result["result"])
    if len(result["result"]) < 500 and not flag:
        return result["result"]
    elif flag and len(result["result"]) < 500 :
        return "ä½ æ˜¯æƒ³é—®ï¼Ÿ\n\n"+result["result"]
    else:
        result = get_first_part(result["result"])
        return remove_trailing_character(result, 'ã€')
    


# # Streamlit åº”ç”¨ç¨‹åºç•Œé¢
def main():
    import os

#     # æŒ‡å®šæ–‡ä»¶è·¯å¾„
#     file_path = 'model/IEITYuan/Yuan2-2B-Mars-hf'

#     # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
#     if not os.path.exists(file_path):
#         print("********** DownLoading the model to local directory **********")
#         from modelscope import snapshot_download
#         model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./model')
    with st.sidebar:
        st.image("cardiorag-logo-without-background.png", use_column_width=True)
        "ğŸ¤– è¯¥é¡¹ç›®ç”± CardioRAG è®¾è®¡å¹¶å¼€å‘"
        "â¤ï¸ æˆ‘ä»¬è¡·å¿ƒå¸Œæœ›æ‚¨èº«å¿ƒå¥åº·"
        "ğŸ§‘â€ğŸ’» [æŸ¥çœ‹é¡¹ç›®æºä»£ç ](https://github.com/peylix/CardioRAG)"
        ""
        "*è¯·æ³¨æ„ï¼Œæœ¬åº”ç”¨ä¸èƒ½å®Œå…¨ä»£æ›¿ä¸“ä¸šåŒ»å¸ˆ*"
        ""
        "[å¤æ—¦åŒ»é™¢æ’è¡Œæ¦œ](https://fdygs.q-health.cn/news2022-1.aspx)"
    
    st.title("çŸ¥å¿ƒæ™ºåŒ» ğŸ§‘â€âš•ï¸")
    # st.title('ğŸ¦œğŸ”— çŸ¥å¿ƒçŸ¥åŒ»AIåŠ©æ‰‹')
    # openai_api_key = st.sidebar.text_input('OpenAI API Key', type='password')

    # æ·»åŠ ä¸€ä¸ªé€‰æ‹©æŒ‰é’®æ¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹
    selected_method = st.sidebar.selectbox("é€‰æ‹©æ¨¡å¼", ["æ™ºèƒ½æ£€ç´¢æ¨¡å¼", "ä¸Šä¸‹æ–‡æ£€ç´¢é—®ç­”æ¨¡å¼", "æ™®é€šæ¨¡å¼", "æ£€ç´¢æ¨¡å¼"])
    # selected_method = st.radio(
    #     "ä½ æƒ³é€‰æ‹©å“ªç§æ¨¡å¼è¿›è¡Œå¯¹è¯ï¼Ÿ",
    #     ["None", "qa_chain", "chat_qa_chain"],
    #     captions = ["ä¸ä½¿ç”¨æ£€ç´¢é—®ç­”çš„æ™®é€šæ¨¡å¼", "ä¸å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼", "å¸¦å†å²è®°å½•çš„æ£€ç´¢é—®ç­”æ¨¡å¼"])
    

    # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²
    if 'messages' not in st.session_state:
        st.session_state.messages = [{
            "role": "assistant",
            "text": "ä½ å¥½ï¼æˆ‘æ˜¯ã€ŒçŸ¥å¿ƒæ™ºåŒ»ã€ï¼Œä½ çš„ç§äººåŒ»ç–—åŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å›ç­”å…³äºé«˜è¡€å‹åŠå† å¿ƒç—…çš„é—®é¢˜ã€‚"
        }]

    messages = st.container(height=400)
    if prompt := st.chat_input("Say something"):
        if False:
            st.info("å‡ºç°äº†ä¸€äº›é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚")
            st.stop()
        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
        st.session_state.messages.append({"role": "user", "text": prompt})
        # answer = get_chat_qa_chain(prompt)
        if selected_method == "æ™®é€šæ¨¡å¼":
            # è°ƒç”¨ respond å‡½æ•°è·å–å›ç­”
            answer = generate_response(prompt)
        elif selected_method == "æ™ºèƒ½æ£€ç´¢æ¨¡å¼":
            answer = get_qa_chain(prompt)
        elif selected_method == "æ£€ç´¢æ¨¡å¼":
            answer = generate_rag(prompt)
        elif selected_method == "ä¸Šä¸‹æ–‡æ£€ç´¢é—®ç­”æ¨¡å¼":
            answer = get_chat_qa_chain(prompt)

        # æ£€æŸ¥å›ç­”æ˜¯å¦ä¸º None
        if answer is not None:
            # å°†LLMçš„å›ç­”æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
            st.session_state.messages.append({"role": "assistant", "text": answer.replace("<eod>", "")})

        # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
        for message in st.session_state.messages:
            if message["role"] == "user":
                messages.chat_message("user").write(message["text"])
            elif message["role"] == "assistant":
                messages.chat_message("assistant").write(message["text"])   

    # æ˜¾ç¤ºè¡¨å•
    if "show_form" not in st.session_state:
        st.session_state.show_form = False

    if st.session_state.show_form:
        with st.form(key="end_conversation_form"):
            # å¤§æ¨¡å‹å­¦ä¹ åŠ©æ‰‹ç”¨æˆ·åé¦ˆè°ƒæŸ¥
            st.subheader("å¤§æ¨¡å‹å­¦ä¹ åŠ©æ‰‹ç”¨æˆ·åé¦ˆè°ƒæŸ¥")
            st.write("æ„Ÿè°¢æ‚¨ä½¿ç”¨æˆ‘ä»¬çš„â€œå¤§æ¨¡å‹å­¦ä¹ åŠ©æ‰‹â€ã€‚ä¸ºäº†å¸®åŠ©æˆ‘ä»¬æ”¹è¿›äº§å“å¹¶æ›´å¥½åœ°æ»¡è¶³æ‚¨çš„éœ€æ±‚ï¼Œè¯·æ‚¨èŠ±å‡ åˆ†é’Ÿæ—¶é—´å¡«å†™æ­¤ç®€çŸ­çš„é—®å·ã€‚æ‚¨çš„æ„è§å¯¹æˆ‘ä»¬éå¸¸é‡è¦ï¼")

            # åŠŸèƒ½æ»¡æ„åº¦
            satisfaction_options = ["éå¸¸æ»¡æ„", "æ»¡æ„", "ä¸­ç­‰", "ä¸æ»¡æ„", "éå¸¸ä¸æ»¡æ„"]

            overall_satisfaction = st.radio("1. æ‚¨å¯¹â€œå¤§æ¨¡å‹å­¦ä¹ åŠ©æ‰‹â€çš„æ•´ä½“æ»¡æ„åº¦å¦‚ä½•ï¼Ÿ", satisfaction_options, horizontal=True)
            response_speed_satisfaction = st.radio("2. æ‚¨å¯¹â€œå¤§æ¨¡å‹å­¦ä¹ åŠ©æ‰‹â€çš„å“åº”é€Ÿåº¦æ»¡æ„å—ï¼Ÿ", satisfaction_options, horizontal=True)
            answer_accuracy_satisfaction = st.radio("3. æ‚¨å¯¹â€œå¤§æ¨¡å‹å­¦ä¹ åŠ©æ‰‹â€çš„ç­”æ¡ˆå‡†ç¡®æ€§æ»¡æ„å—ï¼Ÿ", satisfaction_options, horizontal=True)
            interface_friendlyness_satisfaction = st.radio("4. æ‚¨å¯¹â€œå¤§æ¨¡å‹å­¦ä¹ åŠ©æ‰‹â€çš„ç•Œé¢å‹å¥½æ€§æ»¡æ„å—ï¼Ÿ", satisfaction_options, horizontal=True)

            # æ”¹è¿›å»ºè®®
            improvement_options = ["æ›´å¿«çš„å›ç­”é€Ÿåº¦", "æ›´å‡†ç¡®çš„ç­”æ¡ˆ", "æ›´å‹å¥½çš„ç”¨æˆ·ç•Œé¢", "æ›´å¤šçš„åŠŸèƒ½", "æ›´ä¸ªæ€§åŒ–çš„æœåŠ¡", "å…¶ä»–ï¼ˆè¯·è¯´æ˜ï¼‰"]
            other_improvement = st.multiselect("1. æ‚¨è®¤ä¸ºâ€œå¤§æ¨¡å‹å­¦ä¹ åŠ©æ‰‹â€æœ‰å“ªäº›éœ€è¦æ”¹è¿›çš„åœ°æ–¹ï¼Ÿï¼ˆå¯å¤šé€‰ï¼‰", improvement_options[:-1])
            other_improvement_text = st.text_input("å¦‚æœé€‰æ‹©äº†â€œå…¶ä»–â€ï¼Œè¯·åœ¨è¿™é‡Œè¯´æ˜ï¼š")

            # å…¶ä»–å»ºè®®æˆ–åé¦ˆ
            other_feedback = st.text_area("2. æ‚¨æ˜¯å¦æœ‰å…¶ä»–å»ºè®®æˆ–åé¦ˆï¼Ÿ")

            # æäº¤æŒ‰é’®
            feedback_submit_button = st.form_submit_button("æäº¤åé¦ˆ")

            if feedback_submit_button:

                st.success("æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼")

                feedback_data = {
                    'Overall Satisfaction': overall_satisfaction,
                    'Response Speed Satisfaction': response_speed_satisfaction,
                    'Answer Accuracy Satisfaction': answer_accuracy_satisfaction,
                    'Interface Friendlyness Satisfaction': interface_friendlyness_satisfaction,
                    'Improvement Suggestions': ', '.join(other_improvement),
                    'Other Improvement Text': other_improvement_text,
                    'Other Feedback': other_feedback
                }
                LoggingSave(feedback_data)
    else:
        if st.button("ç»“æŸå¯¹è¯",key = 'end_dialog_button'):
            st.session_state.messages.clear()  
            st.session_state.show_form = True  
         
            # ä½¿ç”¨st.cache_dataæ¥ç®¡ç†å¯¹è¯å†å²
            @st.cache_data(ttl=600)  # ç¼“å­˜5åˆ†é’Ÿ
            def cached_messages():
                return st.session_state.messages

            # æ¸…é™¤ç¼“å­˜
            cached_messages.clear() 


if __name__ == "__main__":
    main()